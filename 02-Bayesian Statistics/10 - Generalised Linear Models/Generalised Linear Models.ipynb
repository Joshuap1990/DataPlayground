{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalised Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most readers of this book will share the experience of fighting with tangled electrical cords. Whether behind a desk or stuffed in a box, cords and cables tend toward tying themselves in knots. Why is this? There is of course real physics at work. But at a descriptive level, the reason is entropy: There are vastly more ways for cords to end up in a knot than for them to remain untied. Events that can happen vastly more ways are more likely.\n",
    "\n",
    "Exploiting entropy is not going to untie your cords. But it will help you solve some problems in choosing distributions. Statistical models force many choices upon us. Some of these choices are distributions that represent uncertainty. We must choose, for each parameter a prior distribution. And we must choose a likelihood function, whch serves as a distribution of data. There are conventional choices, such as wide gaussian priors and the gaussian liklihood of linear regression. These conventional choices work unreasonably well in many circumstances. But very often the conventional choices are not the best choices. Inference can be more powerful when we use all of the information, and doing so usually requires going beyong convention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a researcher wants to build an unconventional model, entropy provides one useful principle to guide the choice of probability distributions. Why? There are three sorts of justifications.\n",
    "\n",
    " - First the distribution with the biggest entropy is the widest and least informative distribution. Choosing a distribtution with the largest entropy means spreading probability as evenly as possible, while still remaining consistent with anything we thingk we know about a process.\n",
    " \n",
    " - Second, nature tends to produce empirical distributions that have high entropy.\n",
    " \n",
    " - Third, regardless of why it works, it tends to work.\n",
    " \n",
    "This chapter serves as a conceptual introduction of Generalised Linear Models and the principle of Maximum entropy. A generalised linear model is much like the linear regressions of previous chapters. It is a model that replaces a parameter of a liklihood function with a linear model. But GLMs needs not use Gaussian Liklihoods. Any liklihood function can be used, and linear models can be attached to and or all of the parameters that describe its shape. The principle of maximum entropy helps us choose liklihood functions, by providing a way to use stated assumptions about constraints on the outcome variable to choose th liklihood function that is the most conservative distribution compatable with the known constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Entropy\n",
    "In brief, we seek a measure of incertainty that satisfies three criteria:\n",
    "\n",
    " - The measure should be continuous\n",
    " - it should increase as the number of possible events increases\n",
    " - it should be additive\n",
    " \n",
    "The principle of maximum entropy applies this measure of uncertainty to the problem of choosing among probability distribtuions. Perhaps the simplest way to state the maximum entropy principle is:\n",
    "\n",
    "    The distribtution that can happen the most ways is also the distribution with the biggest information entropy. The distribution with the biggest entropy is the most conservative distribution that obeys its constraints\n",
    "    \n",
    "    \n",
    "### Gaussian\n",
    "If all we are willing to assume about a collection of measurements is that they have finite variance, then a gaussian distribution represents the most conservative probability distribution to assign those measurements. But very often we are comfortable assuming something more. And in those cases, provided our assumptions are good ones, the principle of maximum entropy leads to distributions other than gaussians\n",
    "\n",
    "### Binomial\n",
    "Back when we introduced bayesian updating by drawing blue and white marbles from a bag. I showed that the liklihood - the relative plausability of an observation - arises from counting the numbers of ways that a given observation could arise, according to out assumptions. The resulting distribtuion is known as the binomial distribution. If only two things can happen, and there is a constant chance p of each across n trials, then the probability of observing y events of type 1 and n-y events of type two - is the binomial formula\n",
    "\n",
    "Now we want to domonstrate that this same distribution has the largest entropy of any distribution that satisfies these constraints.\n",
    "\n",
    " - only two unordered events\n",
    " - constant expected value.\n",
    "\n",
    "\n",
    "When only two un-ordered outcomes are possible - such as blue and white marbles - and the expected numbers of each type of event are assumed to be constant, then the distribution that is most consistent with these constraints is the binomial distribution. This distribution spreads probability out as evenly and conservatively as possible.\n",
    "\n",
    "Second, of course we usually do not know the expected value, but we wish to estimate it. But this is actually the same problem, because assuming the distribution has a constant expected value leads to the binomial distribution as well, but with unknown expected value np, which must be estimated from the data. If only two un-ordered outcomes are possible and you think the process generating them is invariant in time - so that the expected value remains constant at each combination of predictor values - then the distribution that is most conservative is the binomial\n",
    "\n",
    "Third, back in chapter 2, we derived the binomial distribution just be counting how many paths through the garden of forking data were consistent with our assumptions. For each possible composition of a bag of marbles - which corresponds here to each expected value - there is a unique number of ways to realise any possible sequence of data. The likelihoods derived in that way turn out to be exactly the same as the liklihoods we get by maximising entropy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalised linear models\n",
    "\n",
    "The guassian models of previous chapters worked by first assuming a gaussian distribtuion over outcomes. Then, we preplaced the parameter that defines the mean of that distribution, mu, with a linear model. This resulted in liklihood definitions of the sort:\n",
    "\n",
    "y = Normal(mu,sigma)\n",
    "mu = alpha + beta*x\n",
    "\n",
    "For an outcome that is continuous and far from any theoretical maximum or minimum, this sort of gaussian model has maximum entropy. But when the outcome variable is either discrete or bounded, a gaussian likelihood is not the most powerful choice. Consider for example a count outcome, such as the number of blue marbles pulled from a bag. Such a variable is constrained to be zero or a positive integret. Using a Gaussian model with such a variable won't result in a terrifying explosion. But it can't be trusted to do much more than estimate the average count. It certainly can't be trusted to produce sensible predictions, because while you and I know counts can't be negative, a linear regression model does not. So it would happily predict negative values, whenever the mean is close to zero.\n",
    "\n",
    "Luckily, its easier to do better. By using all of our prior knowledge about the outcome variable, usually in the form of constrains on the possible values it can take, we can appeal to maximum entropy for the choice of distribution. Then all we have to do it generalise the linear regression strategy - replce a parameter describing the shape of the likelihood with a linear model - to probability distributions other than gaussian\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
